# Milo Bitcoin Fine-tuning Module ğŸ±âš¡

> Track B: LLM Intelligence - Fine-tuning GPT-OSS-20B for professional Bitcoin quantitative analysis

## ğŸ—ï¸ æ¨¡å—åŒ–æ¶æ„

æœ¬é¡¹ç›®é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œåˆ†ä¸ºä¸‰ä¸ªç‹¬ç«‹æ¨¡å—ï¼š

### ğŸ“Š å½“å‰æ¨¡å—çŠ¶æ€
- âœ… **fine_tune/** - LLMå¾®è°ƒæ¨¡å— (Python 3.11)
  - çŠ¶æ€ï¼šâœ… è®­ç»ƒå®Œæˆï¼Œ122MB LoRAæƒé‡å·²ä¿å­˜
  - ä¾èµ–ï¼šunsloth 2025.9.9 + pytorch 2.8.0+cu128

- âœ… **rag_test/** - RAGçŸ¥è¯†ç³»ç»Ÿ (Python 3.10)
  - çŠ¶æ€ï¼šâœ… granite-doclingé›†æˆå®Œæˆï¼Œæ–‡æ¡£å¤„ç†pipelineå°±ç»ª
  - ä¾èµ–ï¼šdocling + granite + embeddingæ¨¡å‹

- ğŸ”„ **[è®¡åˆ’] vllm/** - æ¨ç†æœåŠ¡æ¨¡å—
  - çŠ¶æ€ï¼šä¸‹ä¸€æ­¥å¼€å‘ç›®æ ‡
  - ä¾èµ–ï¼švllm + fastapi + æ¨¡å‹éƒ¨ç½²å·¥å…·

### ğŸ”— æ¨¡å—åä½œæµç¨‹
```
fine_tune (LoRAæƒé‡) â†’ vllm (æ¨ç†æœåŠ¡) â† rag_test (çŸ¥è¯†æ£€ç´¢)
                            â†“
                    ç»Ÿä¸€JSONæ ¼å¼è¾“å‡º
```

## ğŸ¯ ç›®æ ‡

å°†GPT-OSS-20Bå¾®è°ƒä¸ºä¸“ä¸šçš„Bitcoiné‡åŒ–åˆ†æå¸ˆï¼Œè¾“å‡ºç»“æ„åŒ–çš„JSONæ ¼å¼äº¤æ˜“å»ºè®®ã€‚

## ğŸ—ï¸ ç›®å½•ç»“æ„

```
fine_tune/
â”œâ”€â”€ pyproject.toml              # RTX 5090ä¸“ç”¨ç¯å¢ƒé…ç½®
â”œâ”€â”€ README.md                   # æœ¬æ–‡ä»¶
â”œâ”€â”€ data_preparation/           # æ•°æ®é¢„å¤„ç†
â”‚   â”œâ”€â”€ download_datasets.py    # ä¸‹è½½HuggingFaceæ•°æ®é›†
â”‚   â”œâ”€â”€ data_formatter.py       # ç»Ÿä¸€æ•°æ®æ ¼å¼
â”‚   â””â”€â”€ data_mixer.py           # 85%-10%-5%æ•°æ®æ··åˆ
â”œâ”€â”€ training_scripts/           # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ unsloth_trainer.py      # ä¸»è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ config.yaml             # è®­ç»ƒé…ç½®
â”‚   â””â”€â”€ monitor.py              # è®­ç»ƒç›‘æ§
â”œâ”€â”€ model_export/               # æ¨¡å‹å¯¼å‡º
â”‚   â”œâ”€â”€ export_for_vllm.py      # å¯¼å‡ºvLLMå…¼å®¹æ ¼å¼
â”‚   â””â”€â”€ model_validator.py      # æ¨¡å‹éªŒè¯
â”œâ”€â”€ configs/                    # é…ç½®æ–‡ä»¶
â”œâ”€â”€ logs/                       # è®­ç»ƒæ—¥å¿—
â””â”€â”€ checkpoints/                # æ¨¡å‹æ£€æŸ¥ç‚¹
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…Fine-tuneæ¨¡å—ä¾èµ–

```bash
# è®¾ç½®RTX 5090ç¯å¢ƒå˜é‡
export TORCH_CUDA_ARCH_LIST="12.0"
export CUDA_VISIBLE_DEVICES=0

# LLMå¾®è°ƒæ¨¡å— (æœ¬æ¨¡å—)
cd fine_tune
source .venv/bin/activate
uv sync  # å®‰è£…å¾®è°ƒç›¸å…³ä¾èµ– (unsloth + pytorch + transformers)
```

### 2. å®‰è£…RAGæ¨¡å—ä¾èµ– (å¹¶è¡Œå¼€å‘)

```bash
# RAGçŸ¥è¯†ç³»ç»Ÿæ¨¡å—
cd rag_test
source .venv/bin/activate
uv sync  # å®‰è£…RAGç›¸å…³ä¾èµ– (granite-docling + embedding)
```

### 3. éªŒè¯RTX 5090æ”¯æŒ

```bash
python -c "
import torch
print(f'CUDA Available: {torch.cuda.is_available()}')
print(f'Device Name: {torch.cuda.get_device_name()}')
print(f'Device Capability: {torch.cuda.get_device_capability()}')
print(f'Memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB')
"
```

é¢„æœŸè¾“å‡ºï¼š
```
CUDA Available: True
Device Name: NVIDIA GeForce RTX 5090
Device Capability: (12, 0)
Memory: 32.6GB
```

### 4. æ•°æ®å‡†å¤‡ (âœ… å·²å®Œæˆ)

```bash
# âœ… æ•°æ®é›†å·²ä¸‹è½½å¹¶é¢„å¤„ç†å®Œæˆ
# final_data/ ç›®å½•åŒ…å«:
# - train.jsonl: 18,719æ ·æœ¬ (72.8MB)
# - validation.jsonl: 2,335æ ·æœ¬ (9.1MB)
# - test.jsonl: 1,095æ ·æœ¬ (4.1MB)
```

### 5. å¾®è°ƒè®­ç»ƒ (âœ… å·²å®Œæˆ)

```bash
# âœ… è®­ç»ƒå·²å®Œæˆ - ä½¿ç”¨ä»¥ä¸‹è„šæœ¬è¿›è¡Œçš„è®­ç»ƒ
python training_scripts/simple_trainer.py

# âœ… è®­ç»ƒç»“æœ:
# - è®­ç»ƒæ—¶é—´: 1.65å°æ—¶ (3 epochs)
# - LoRAæƒé‡: 122MB (checkpoints/adapter_model.safetensors)
# - æŸå¤±æ”¶æ•›: 1.32 â†’ 1.25
# - WandBç›‘æ§: å·²è®°å½•å®Œæ•´è®­ç»ƒè¿‡ç¨‹
```

## ğŸ“Š æ•°æ®é›†ç­–ç•¥

åŸºäºsession logçš„åˆ†æç»“æœï¼š

### é€‰å®šæ•°æ®é›†
1. **bitcoin-llm-finetuning-dataset_new**: 2,301æ ·æœ¬ (99.3%é«˜è´¨é‡)
2. **bitcoin-sllm-instruct_v2**: 4,290æ ·æœ¬ (99.9%é«˜è´¨é‡)

### æ•°æ®æ··åˆæ¯”ä¾‹
- **85% Bitcoinä¸“ä¸šæ•°æ®**: 5,610æ ·æœ¬ - æ ¸å¿ƒä¸“ä¸šèƒ½åŠ›
- **10% æ•°å­¦æ¨ç†æ•°æ®**: 660æ ·æœ¬ - è®¡ç®—èƒ½åŠ›ä¿æŒ
- **5% é€»è¾‘æ¨ç†æ•°æ®**: 330æ ·æœ¬ - åŸºç¡€æ¨ç†èƒ½åŠ›

### ç›®æ ‡è¾“å‡ºæ ¼å¼
```json
{
  "action": "BUY|SELL|HOLD",
  "confidence": 72,
  "current_price": 109453.00,
  "stop_loss": 105200.00,
  "take_profit": 116800.00,
  "forecast_10d": [109800, 111200, ...],
  "analysis": "æŠ€æœ¯é¢åˆ†ææ–‡æœ¬",
  "risk_score": 0.31,
  "technical_indicators": {...}
}
```

## âš™ï¸ è®­ç»ƒé…ç½®

### å®é™…è®­ç»ƒé…ç½® (å·²éªŒè¯æˆåŠŸ)
```yaml
# âœ… æˆåŠŸä½¿ç”¨çš„é…ç½®
model_name: "openai/gpt-oss-20b"  # å®é™…ä½¿ç”¨æ¨¡å‹
max_seq_length: 2048

# LoRAå‚æ•° (å®é™…æˆåŠŸé…ç½®)
lora_r: 64                    # rank
lora_alpha: 128               # 2*ré…æ¯”
lora_dropout: 0.1
target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

# è®­ç»ƒå‚æ•° (å®é™…ä½¿ç”¨)
per_device_train_batch_size: 4
gradient_accumulation_steps: 8
learning_rate: 2e-4           # æˆåŠŸçš„å­¦ä¹ ç‡
num_train_epochs: 3
warmup_steps: 100

# å†…å­˜ä¼˜åŒ–
gradient_checkpointing: "unsloth"  # Unslothä¼˜åŒ–
optim: "adamw_8bit"
fp16: true
dataloader_pin_memory: false

# å®é™…ä¾èµ–ç‰ˆæœ¬
software_versions:
  unsloth: "2025.9.9"
  torch: "2.8.0+cu128"
  transformers: "4.56.2"
  peft: "0.17.1"
```

## ğŸ“ˆ å®é™…æ€§èƒ½ç»“æœ

### âœ… è®­ç»ƒæ€§èƒ½ (RTX 5090)
- **æ€»è®­ç»ƒæ—¶é—´**: 1.65å°æ—¶ (3 epochs) - æ¯”é¢„æœŸå¿«7å€ï¼
- **è®­ç»ƒé€Ÿåº¦**: 3.145 samples/second
- **æ£€æŸ¥ç‚¹ä¿å­˜**: æ¯200æ­¥ (checkpoint-200, 400, 585)
- **å†…å­˜æ•ˆç‡**: ä¼˜äºé¢„æœŸï¼ŒRTX 5090æ€§èƒ½å……è¶³

### âœ… æ¨¡å‹è§„æ¨¡
- **LoRAæƒé‡**: 122MB (vs åŸºç¡€æ¨¡å‹~20GB)
- **å‹ç¼©æ¯”**: 99.4% å‚æ•°å‡å°‘
- **checkpointsæ€»å¤§å°**: 779MB (åŒ…å«3ä¸ªæ£€æŸ¥ç‚¹)
- **éƒ¨ç½²æ–‡ä»¶**: ä»…éœ€149MB (ç”Ÿäº§ç¯å¢ƒ)

## ğŸ”— ä¸ä¸»æ¡†æ¶é›†æˆ

è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹å°†é›†æˆåˆ°ï¼š
```
ZJ_Volume/
â”œâ”€â”€ milo_bitcoin_main.py      # ä¸»æ¡†æ¶ - æ›¿æ¢TODOæ¨¡å‹
â”œâ”€â”€ fine_tune/                # æœ¬ç›®å½•
â”œâ”€â”€ rag_system/               # RAGçŸ¥è¯†ç³»ç»Ÿ (å¹¶è¡Œå¼€å‘)
â””â”€â”€ integration/              # é›†æˆéƒ¨ç½²
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **sm_120ä¸æ”¯æŒé”™è¯¯**
```bash
# ç¡®ä¿ä½¿ç”¨CUDA 12.8ï¼Œä¸æ˜¯13.0
pip install torch --extra-index-url https://download.pytorch.org/whl/cu128
export TORCH_CUDA_ARCH_LIST="12.0"
```

2. **å†…å­˜ä¸è¶³**
```bash
# å‡å°‘æ‰¹æ¬¡å¤§å°
per_device_train_batch_size: 2
gradient_accumulation_steps: 16
```

3. **Unslothå®‰è£…é—®é¢˜**
```bash
# é‡æ–°å®‰è£…
pip uninstall unsloth -y
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
```

## ğŸ“ é¡¹ç›®è¿›åº¦

### âœ… å·²å®Œæˆä»»åŠ¡
1. âœ… ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ
2. âœ… PyTorch sm_120å…¼å®¹æ€§éªŒè¯ (CUDA 12.8 + RTX 5090)
3. âœ… è®­ç»ƒç¯å¢ƒé…ç½®å®Œæˆ (pyproject.tomlä¿®å¤)
4. âœ… æ•°æ®é›†ä¸‹è½½å’Œé¢„å¤„ç† (18,719è®­ç»ƒæ ·æœ¬)
5. âœ… GPT-OSS-20B LoRAå¾®è°ƒè®­ç»ƒ (122MBæƒé‡)
6. âœ… è®­ç»ƒç›‘æ§å’Œæ£€æŸ¥ç‚¹ä¿å­˜

### ğŸ”„ å½“å‰ä»»åŠ¡ (ä¸‹ä¸€æ­¥)
7. ğŸ”„ **æ¨¡å‹è´¨é‡è¯„ä¼°** - æ¨ç†æµ‹è¯•å’Œæ€§èƒ½éªŒè¯
8. â¸ï¸ **vLLMéƒ¨ç½²é…ç½®** - æ¨ç†æœåŠ¡å™¨æ­å»º
9. â¸ï¸ **ä¸ä¸»æ¡†æ¶é›†æˆ** - ç»Ÿä¸€APIæ¥å£è®¾è®¡

### ğŸ¯ æ¨¡å—åŒ–é›†æˆè§„åˆ’
- **fine_tune** (æœ¬æ¨¡å—): âœ… è®­ç»ƒå®Œæˆ
- **rag_test**: âœ… æ–‡æ¡£å¤„ç†pipelineå°±ç»ª
- **vllm** (è®¡åˆ’): æ¨ç†æœåŠ¡å’Œä¸¤æ¨¡å—æ•´åˆ

---

**è®°ä½**: è¿™æ˜¯å¹¶è¡Œå¼€å‘çš„Track B (LLMå¾®è°ƒ)ï¼Œä¸Track A (RAGç³»ç»Ÿ)ç‹¬ç«‹è¿›è¡Œï¼Œæœ€ç»ˆåœ¨integrationå±‚æ±‡åˆã€‚ğŸš€