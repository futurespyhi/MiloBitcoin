#!/usr/bin/env python3
"""
Milo Bitcoin - Unsloth Trainer
RTX 5090ä¼˜åŒ–çš„GPT-OSS-20Bå¾®è°ƒè„šæœ¬

é…ç½®è¯´æ˜:
- æ¨¡å‹: microsoft/DialoGPT-medium (ä½œä¸ºGPT-OSS-20Bæ›¿ä»£)
- LoRA: rank=64, alpha=128 (2*ré…æ¯”)
- æ‰¹æ¬¡å¤§å°: 4 (RTX 5090 32GBä¼˜åŒ–)
- æ¢¯åº¦ç´¯ç§¯: 8æ­¥ (æœ‰æ•ˆæ‰¹æ¬¡32)
- å­¦ä¹ ç‡: 2e-4 (Unslothæ”¯æŒæ›´é«˜LR)
"""

import os
import sys
import json
import torch
import wandb
from pathlib import Path
from typing import Dict, List, Optional
import pandas as pd
from datasets import Dataset
from transformers import TrainingArguments
from trl import SFTTrainer
import logging
from rich.console import Console
from rich.progress import Progress
from rich.panel import Panel
from rich.table import Table
from rich.text import Text

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
console = Console()

# ç¯å¢ƒæ£€æŸ¥
console.print("ğŸ”§ æ£€æŸ¥Unslothå’ŒGPUç¯å¢ƒ...")
try:
    from unsloth import FastLanguageModel
    from unsloth import is_bfloat16_supported
    console.print("âœ… Unslothå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    console.print(f"âŒ Unslothå¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# GPUæ£€æŸ¥
if not torch.cuda.is_available():
    console.print("âŒ CUDAä¸å¯ç”¨")
    sys.exit(1)

gpu_name = torch.cuda.get_device_name()
gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
console.print(f"âœ… GPU: {gpu_name} ({gpu_memory:.1f}GB)")

class BitcoinUnslothTrainer:
    """Bitcoinä¸“ä¸šå¾®è°ƒè®­ç»ƒå™¨ - RTX 5090ä¼˜åŒ–"""

    def __init__(self,
                 data_dir: str = "final_data",
                 output_dir: str = "checkpoints",
                 config_file: Optional[str] = None):

        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        # è®­ç»ƒé…ç½® - RTX 5090ä¼˜åŒ–
        self.config = {
            # æ¨¡å‹é…ç½®
            "model_name": "openai/gpt-oss-20b",  # çœŸæ­£çš„GPT-OSS-20B (21Bå‚æ•°)
            "max_seq_length": 2048,  # è€ƒè™‘åˆ°é•¿input
            "dtype": None,  # è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç±»å‹
            "load_in_4bit": True,  # 4-bité‡åŒ–èŠ‚çœå†…å­˜

            # LoRAé…ç½® (32GB VRAMå¯ä»¥æ›´æ¿€è¿›)
            "lora_r": 64,  # rank
            "lora_alpha": 128,  # 2*ré…æ¯”
            "lora_dropout": 0.1,
            "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj"],  # GPT-OSS-20B attentionæ¨¡å—
            "lora_bias": "none",
            "task_type": "CAUSAL_LM",

            # è®­ç»ƒå‚æ•°
            "per_device_train_batch_size": 4,
            "per_device_eval_batch_size": 4,
            "gradient_accumulation_steps": 8,  # æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: 4*8=32
            "learning_rate": 2e-4,  # Unslothæ”¯æŒæ›´é«˜å­¦ä¹ ç‡
            "num_train_epochs": 3,
            "max_steps": -1,  # ä½¿ç”¨epochsè€Œésteps
            "warmup_steps": 100,
            "weight_decay": 0.01,

            # å†…å­˜ä¼˜åŒ–
            "gradient_checkpointing": True,
            "optim": "adamw_8bit",
            "fp16": not is_bfloat16_supported(),
            "bf16": is_bfloat16_supported(),
            "dataloader_pin_memory": False,
            "dataloader_num_workers": 4,

            # ç›‘æ§å’Œä¿å­˜
            "eval_steps": 100,
            "save_steps": 200,
            "logging_steps": 10,
            "evaluation_strategy": "steps",
            "save_strategy": "steps",
            "load_best_model_at_end": True,
            "metric_for_best_model": "eval_loss",
            "greater_is_better": False,
            "save_total_limit": 3,

            # wandbé…ç½®
            "report_to": "wandb",
            "run_name": f"milo-bitcoin-{pd.Timestamp.now().strftime('%Y%m%d-%H%M')}",
        }

        # åŠ è½½è‡ªå®šä¹‰é…ç½®
        if config_file and Path(config_file).exists():
            with open(config_file, 'r') as f:
                custom_config = json.load(f)
                self.config.update(custom_config)
                console.print(f"âœ… åŠ è½½è‡ªå®šä¹‰é…ç½®: {config_file}")

    def load_datasets(self) -> Dict[str, Dataset]:
        """åŠ è½½è®­ç»ƒæ•°æ®é›†"""
        console.print("ğŸ“ åŠ è½½è®­ç»ƒæ•°æ®é›†...")

        datasets = {}

        # è®­ç»ƒé›†
        train_file = self.data_dir / "train.jsonl"
        if not train_file.exists():
            raise FileNotFoundError(f"è®­ç»ƒæ–‡ä»¶ä¸å­˜åœ¨: {train_file}")

        train_data = []
        with open(train_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    train_data.append(json.loads(line.strip()))

        datasets["train"] = Dataset.from_pandas(pd.DataFrame(train_data))
        console.print(f"  âœ… è®­ç»ƒé›†: {len(train_data):,} æ ·æœ¬")

        # éªŒè¯é›†
        val_file = self.data_dir / "validation.jsonl"
        if val_file.exists():
            val_data = []
            with open(val_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        val_data.append(json.loads(line.strip()))

            datasets["validation"] = Dataset.from_pandas(pd.DataFrame(val_data))
            console.print(f"  âœ… éªŒè¯é›†: {len(val_data):,} æ ·æœ¬")

        return datasets

    def setup_model_and_tokenizer(self):
        """è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨"""
        console.print(f"ğŸ¤– åŠ è½½æ¨¡å‹: {self.config['model_name']}")

        # ä½¿ç”¨Unslothå¿«é€ŸåŠ è½½
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=self.config["model_name"],
            max_seq_length=self.config["max_seq_length"],
            dtype=self.config["dtype"],
            load_in_4bit=self.config["load_in_4bit"]
        )

        # é…ç½®LoRA
        model = FastLanguageModel.get_peft_model(
            model,
            r=self.config["lora_r"],
            alpha=self.config["lora_alpha"],
            target_modules=self.config["target_modules"],
            lora_dropout=self.config["lora_dropout"],
            bias=self.config["lora_bias"],
            use_gradient_checkpointing=self.config["gradient_checkpointing"],
            random_state=42,
        )

        # é…ç½®åˆ†è¯å™¨
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        console.print("âœ… æ¨¡å‹å’ŒLoRAé…ç½®å®Œæˆ")

        # æ˜¾ç¤ºå¯è®­ç»ƒå‚æ•°
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        console.print(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {total_params:,} ({trainable_params/total_params*100:.2f}%)")

        return model, tokenizer

    def format_prompt(self, sample: Dict) -> str:
        """æ ¼å¼åŒ–è®­ç»ƒæ ·æœ¬ä¸ºGPT-OSS-20Bçš„harmonyæ ¼å¼"""
        instruction = sample["instruction"]
        input_text = sample["input"]
        output = sample["output"]

        # ä½¿ç”¨GPT-OSS-20Bçš„harmony responseæ ¼å¼
        if input_text.strip():
            prompt = f"<|user|>\n{instruction}\n\n{input_text}<|end|>\n<|assistant|>\n{output}<|end|>"
        else:
            prompt = f"<|user|>\n{instruction}<|end|>\n<|assistant|>\n{output}<|end|>"

        return prompt

    def setup_trainer(self, model, tokenizer, datasets):
        """è®¾ç½®SFTè®­ç»ƒå™¨"""
        console.print("ğŸ‹ï¸ è®¾ç½®SFTè®­ç»ƒå™¨...")

        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=str(self.output_dir),
            per_device_train_batch_size=self.config["per_device_train_batch_size"],
            per_device_eval_batch_size=self.config["per_device_eval_batch_size"],
            gradient_accumulation_steps=self.config["gradient_accumulation_steps"],
            learning_rate=self.config["learning_rate"],
            num_train_epochs=self.config["num_train_epochs"],
            max_steps=self.config["max_steps"],
            warmup_steps=self.config["warmup_steps"],
            weight_decay=self.config["weight_decay"],
            optim=self.config["optim"],
            fp16=self.config["fp16"],
            bf16=self.config["bf16"],
            gradient_checkpointing=self.config["gradient_checkpointing"],
            dataloader_pin_memory=self.config["dataloader_pin_memory"],
            dataloader_num_workers=self.config["dataloader_num_workers"],
            eval_steps=self.config["eval_steps"],
            save_steps=self.config["save_steps"],
            logging_steps=self.config["logging_steps"],
            eval_strategy=self.config["evaluation_strategy"],  # æ–°ç‰ˆæœ¬ä½¿ç”¨eval_strategy
            save_strategy=self.config["save_strategy"],
            load_best_model_at_end=self.config["load_best_model_at_end"],
            metric_for_best_model=self.config["metric_for_best_model"],
            greater_is_better=self.config["greater_is_better"],
            save_total_limit=self.config["save_total_limit"],
            report_to=self.config["report_to"],
            run_name=self.config["run_name"],
            remove_unused_columns=False,
            max_length=self.config["max_seq_length"],  # åœ¨TrainingArgumentsä¸­è®¾ç½®max_length
        )

        # SFTè®­ç»ƒå™¨ - æ–°ç‰ˆæœ¬TRLå…¼å®¹
        trainer = SFTTrainer(
            model=model,
            args=training_args,
            train_dataset=datasets["train"],
            eval_dataset=datasets.get("validation"),
            processing_class=tokenizer,  # æ–°ç‰ˆæœ¬ä½¿ç”¨processing_class
            formatting_func=self.format_prompt,
            packing=False,  # å¯¹äºé•¿åºåˆ—ï¼Œå…³é—­packing
        )

        console.print("âœ… SFTè®­ç»ƒå™¨é…ç½®å®Œæˆ")
        return trainer

    def monitor_training(self, trainer):
        """è®­ç»ƒè¿‡ç¨‹ç›‘æ§"""
        console.print("ğŸ“Š å¼€å§‹è®­ç»ƒç›‘æ§...")

        # æ˜¾ç¤ºè®­ç»ƒé…ç½®æ‘˜è¦
        config_table = Table(title="ğŸ¯ è®­ç»ƒé…ç½®æ‘˜è¦")
        config_table.add_column("å‚æ•°", style="cyan")
        config_table.add_column("å€¼", style="yellow")

        key_configs = [
            ("æ¨¡å‹", self.config["model_name"]),
            ("æœ€å¤§åºåˆ—é•¿åº¦", self.config["max_seq_length"]),
            ("LoRA Rank", self.config["lora_r"]),
            ("LoRA Alpha", self.config["lora_alpha"]),
            ("æ‰¹æ¬¡å¤§å°", f"{self.config['per_device_train_batch_size']} x {self.config['gradient_accumulation_steps']} = {self.config['per_device_train_batch_size'] * self.config['gradient_accumulation_steps']}"),
            ("å­¦ä¹ ç‡", self.config["learning_rate"]),
            ("è®­ç»ƒè½®æ•°", self.config["num_train_epochs"]),
            ("ä¼˜åŒ–å™¨", self.config["optim"]),
            ("ç²¾åº¦", "BF16" if self.config["bf16"] else "FP16"),
        ]

        for param, value in key_configs:
            config_table.add_row(param, str(value))

        console.print(config_table)

    def save_training_metadata(self):
        """ä¿å­˜è®­ç»ƒå…ƒæ•°æ®"""
        metadata = {
            "config": self.config,
            "gpu_info": {
                "name": torch.cuda.get_device_name(),
                "memory_gb": torch.cuda.get_device_properties(0).total_memory / 1e9,
                "compute_capability": torch.cuda.get_device_capability()
            },
            "training_start": pd.Timestamp.now().isoformat(),
            "unsloth_version": "2025.6.0+",  # å‡è®¾ç‰ˆæœ¬
            "pytorch_version": torch.__version__,
        }

        metadata_file = self.output_dir / "training_metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        console.print(f"âœ… è®­ç»ƒå…ƒæ•°æ®å·²ä¿å­˜: {metadata_file}")

    def run_training(self):
        """æ‰§è¡Œå®Œæ•´è®­ç»ƒæµç¨‹"""
        console.clear()
        console.print("ğŸš€ [bold cyan]Milo Bitcoin - Unsloth Trainer[/bold cyan]")
        console.print("RTX 5090ä¼˜åŒ–çš„Bitcoiné‡åŒ–åˆ†æå¸ˆå¾®è°ƒ\n")

        try:
            # 1. åŠ è½½æ•°æ®é›†
            datasets = self.load_datasets()

            # 2. è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨
            model, tokenizer = self.setup_model_and_tokenizer()

            # 3. è®¾ç½®è®­ç»ƒå™¨
            trainer = self.setup_trainer(model, tokenizer, datasets)

            # 4. è®­ç»ƒç›‘æ§
            self.monitor_training(trainer)

            # 5. ä¿å­˜å…ƒæ•°æ®
            self.save_training_metadata()

            # 6. åˆå§‹åŒ–wandb (å¦‚æœé…ç½®)
            if self.config["report_to"] == "wandb":
                console.print("ğŸ”— åˆå§‹åŒ–Wandbç›‘æ§...")
                wandb.init(
                    project="milo-bitcoin-finetuning",
                    name=self.config["run_name"],
                    config=self.config
                )

            # 7. å¼€å§‹è®­ç»ƒ
            console.print("\nğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
            train_result = trainer.train()

            # 8. ä¿å­˜æœ€ç»ˆæ¨¡å‹
            console.print("ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
            trainer.save_model()

            # 9. è®­ç»ƒæ€»ç»“
            console.print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")

            summary_table = Table(title="ğŸ“ˆ è®­ç»ƒæ€»ç»“")
            summary_table.add_column("æŒ‡æ ‡", style="cyan")
            summary_table.add_column("å€¼", style="yellow")

            summary_table.add_row("æœ€ç»ˆæŸå¤±", f"{train_result.training_loss:.6f}")
            summary_table.add_row("è®­ç»ƒæ­¥æ•°", f"{train_result.global_step:,}")
            summary_table.add_row("æ¨¡å‹ä¿å­˜ä½ç½®", str(self.output_dir))

            console.print(summary_table)

            # ä¸‹ä¸€æ­¥æŒ‡å¯¼
            next_steps = Text()
            next_steps.append("ğŸ¯ ä¸‹ä¸€æ­¥æ“ä½œ:\n\n", style="bold green")
            next_steps.append("1. å¯¼å‡ºæ¨¡å‹: python model_export/export_for_vllm.py\n", style="white")
            next_steps.append("2. æµ‹è¯•æ¨¡å‹: python model_export/model_validator.py\n", style="white")
            next_steps.append("3. é›†æˆåˆ°Milo: æ›¿æ¢ä¸»æ¡†æ¶æ¨¡å‹è·¯å¾„\n", style="white")

            console.print(Panel(next_steps, title="âœ¨ å¾®è°ƒå®Œæˆ", border_style="green"))

            return True

        except Exception as e:
            logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
            console.print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            return False

        finally:
            if wandb.run:
                wandb.finish()

if __name__ == "__main__":
    # è®¾ç½®CUDAç¯å¢ƒ
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"

    # åˆ›å»ºè®­ç»ƒå™¨å¹¶æ‰§è¡Œ
    trainer = BitcoinUnslothTrainer()
    success = trainer.run_training()

    if success:
        console.print("\nğŸ‰ Bitcoiné‡åŒ–åˆ†æå¸ˆå¾®è°ƒæˆåŠŸå®Œæˆ!")
    else:
        console.print("\nâŒ å¾®è°ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
        sys.exit(1)