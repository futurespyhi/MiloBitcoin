#!/usr/bin/env python3
"""
Milo Bitcoin - Simple Unsloth Trainer
ç®€åŒ–ç‰ˆè®­ç»ƒè„šæœ¬ï¼Œé¿å¼€ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
"""

import os
import json
import torch
import pandas as pd
from pathlib import Path
from datasets import Dataset
from transformers import TrainingArguments
from rich.console import Console

console = Console()

# å¯¼å…¥Unsloth
try:
    from unsloth import FastLanguageModel
    from unsloth import is_bfloat16_supported
    from unsloth.chat_templates import get_chat_template
    console.print("âœ… Unslothå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    console.print(f"âŒ Unslothå¯¼å…¥å¤±è´¥: {e}")
    exit(1)

def load_training_data():
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    console.print("ğŸ“ åŠ è½½è®­ç»ƒæ•°æ®...")

    train_file = Path("final_data/train.jsonl")
    val_file = Path("final_data/validation.jsonl")

    # åŠ è½½è®­ç»ƒæ•°æ®
    train_data = []
    with open(train_file, 'r') as f:
        for line in f:
            if line.strip():
                train_data.append(json.loads(line))

    # åŠ è½½éªŒè¯æ•°æ®
    val_data = []
    if val_file.exists():
        with open(val_file, 'r') as f:
            for line in f:
                if line.strip():
                    val_data.append(json.loads(line))

    console.print(f"âœ… è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
    console.print(f"âœ… éªŒè¯é›†: {len(val_data)} æ ·æœ¬")

    return train_data, val_data

def format_prompts(examples):
    """æ ¼å¼åŒ–è®­ç»ƒæ ·æœ¬"""
    texts = []
    for instruction, input_text, output in zip(examples["instruction"], examples["input"], examples["output"]):
        # ä½¿ç”¨GPT-OSS-20Bçš„æ ¼å¼
        if input_text.strip():
            text = f"<|user|>\n{instruction}\n\n{input_text}<|end|>\n<|assistant|>\n{output}<|end|>"
        else:
            text = f"<|user|>\n{instruction}<|end|>\n<|assistant|>\n{output}<|end|>"
        texts.append(text)
    return {"text": texts}

def main():
    console.clear()
    console.print("ğŸš€ [bold cyan]Milo Bitcoin - Simple Trainer[/bold cyan]")
    console.print("GPT-OSS-20Bå¾®è°ƒ (ç®€åŒ–ç‰ˆ)\n")

    # 1. åŠ è½½æ•°æ®
    train_data, val_data = load_training_data()

    # 2. åŠ è½½æ¨¡å‹
    console.print("ğŸ¤– åŠ è½½GPT-OSS-20Bæ¨¡å‹...")
    max_seq_length = 2048
    dtype = None
    load_in_4bit = True

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="openai/gpt-oss-20b",
        max_seq_length=max_seq_length,
        dtype=dtype,
        load_in_4bit=load_in_4bit,
    )

    # 3. é…ç½®LoRA
    console.print("âš™ï¸ é…ç½®LoRA...")
    model = FastLanguageModel.get_peft_model(
        model,
        r=64,
        alpha=128,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_dropout=0.1,
        bias="none",
        use_gradient_checkpointing="unsloth",
        random_state=42,
    )

    # æ˜¾ç¤ºå¯è®­ç»ƒå‚æ•°
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    console.print(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {total_params:,} ({trainable_params/total_params*100:.2f}%)")

    # 4. å‡†å¤‡æ•°æ®é›†
    console.print("ğŸ“‹ å‡†å¤‡æ•°æ®é›†...")
    train_dataset = Dataset.from_list(train_data)
    val_dataset = Dataset.from_list(val_data) if val_data else None

    train_dataset = train_dataset.map(format_prompts, batched=True)
    if val_dataset:
        val_dataset = val_dataset.map(format_prompts, batched=True)

    # 5. ä½¿ç”¨Unslothçš„SFTTrainer
    console.print("ğŸ‹ï¸ è®¾ç½®è®­ç»ƒå™¨...")
    from trl import SFTTrainer
    from transformers import DataCollatorForSeq2Seq

    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        dataset_text_field="text",
        max_seq_length=max_seq_length,
        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),
        dataset_num_proc=2,
        packing=False,
        args=TrainingArguments(
            per_device_train_batch_size=4,
            gradient_accumulation_steps=8,
            warmup_steps=100,
            num_train_epochs=3,
            learning_rate=2e-4,
            fp16=not is_bfloat16_supported(),
            bf16=is_bfloat16_supported(),
            logging_steps=10,
            optim="adamw_8bit",
            weight_decay=0.01,
            lr_scheduler_type="linear",
            seed=42,
            output_dir="checkpoints",
            save_steps=200,
            eval_steps=100,
            eval_strategy="steps",
            save_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            report_to="wandb",
            run_name=f"milo-bitcoin-{pd.Timestamp.now().strftime('%Y%m%d-%H%M')}",
        ),
    )

    # 6. å¼€å§‹è®­ç»ƒ
    console.print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    console.print("ğŸ“Š è®­ç»ƒç›‘æ§: https://wandb.ai/zgu17/huggingface")

    # åˆå§‹åŒ–wandb
    import wandb
    wandb.init(
        project="milo-bitcoin-finetuning",
        name=f"simple-trainer-{pd.Timestamp.now().strftime('%Y%m%d-%H%M')}",
        config={
            "model": "openai/gpt-oss-20b",
            "lora_r": 64,
            "lora_alpha": 128,
            "batch_size": 4,
            "grad_accum": 8,
            "learning_rate": 2e-4,
            "epochs": 3,
        }
    )

    # è®­ç»ƒ
    trainer_stats = trainer.train()

    # 7. ä¿å­˜æ¨¡å‹
    console.print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
    trainer.save_model()

    # 8. ä¿å­˜ç»Ÿè®¡
    stats = {
        "training_loss": trainer_stats.training_loss,
        "global_step": trainer_stats.global_step,
        "metrics": trainer_stats.metrics,
    }

    with open("checkpoints/training_stats.json", "w") as f:
        json.dump(stats, f, indent=2)

    console.print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    console.print(f"ğŸ“Š æœ€ç»ˆæŸå¤±: {trainer_stats.training_loss:.6f}")
    console.print(f"ğŸ”¢ å…¨å±€æ­¥æ•°: {trainer_stats.global_step}")
    console.print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜åœ¨: checkpoints/")

    wandb.finish()

if __name__ == "__main__":
    main()